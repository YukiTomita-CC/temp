import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def main(model_name):
    if model_name not in ["calm3", "tanuki", "qwen"]:
        print("model_name must be one of 'calm3', 'tanuki', 'qwen'.")
        return
    
    model_name = f"models/{model_name}"

    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype="auto")
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    import json


    conversation_file_path = 'memo1.json'
    persona_list_file_path = 'user_persona.json'

    with open(conversation_file_path, 'r', encoding='utf-8') as f:
        conversation_data = json.load(f)

    with open(persona_list_file_path, 'r', encoding='utf-8') as f:
        persona_list = json.load(f)

    def generate_prompt(conversation_data, persona_list, target_persona_id):
        old_persona_id = conversation_data.get('user_persona_id')
        theme = conversation_data.get('theme')
        
        old_persona = next((p for p in persona_list if p['id'] == old_persona_id), None)
        new_persona = next((p for p in persona_list if p['id'] == target_persona_id), None)
        
        conversation_lines = []
        for turn in conversation_data['conversations']:
            role = 'User' if turn['role'] == 'user' else 'Assistant'
            content = turn['content']
            conversation_lines.append(f"{role}: {content}")
        conversation_text = '\n'.join(conversation_lines)
        
        def format_persona(persona, persona_name):
            items = [f"**{persona_name}ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒšãƒ«ã‚½ãƒŠ**ï¼š\n"]
            for key, value in persona.items():
                if key != 'id':
                    items.append(f"- {key}: {value}")
            return '\n'.join(items)
        
        old_persona_text = format_persona(old_persona, "æ—¢å­˜")
        new_persona_text = format_persona(new_persona, "æ–°è¦")
        
        return conversation_text, old_persona_text, new_persona_text, old_persona.get('åå‰'), new_persona.get('åå‰')

    target_persona_id = 2
    conversation_text, old_persona_text, new_persona_text, old_persona_name, new_persona_name = generate_prompt(conversation_data, persona_list, target_persona_id)

    prompt = f"""**æŒ‡ç¤º**ï¼š

    ä»¥ä¸‹ã®æ—¢å­˜ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒšãƒ«ã‚½ãƒŠã€Œ{old_persona_name}ã€ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚ã“ã®ä¼šè©±ã‚’ã€æ–°ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒšãƒ«ã‚½ãƒŠã€Œ{new_persona_name}ã€ã«åˆã‚ã›ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€å†…å®¹ã¨è¨€è‘‰é£ã„ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚**ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ç™ºè¨€ã‚‚ã€ä¼šè©±ã®è‡ªç„¶ã•ã‚’ä¿ã¤ãŸã‚ã«å¿…è¦ãªå ´åˆã®ã¿æœ€å°é™ã®å¤‰æ›´ã‚’è¡Œã£ã¦ãã ã•ã„**ã€‚ãƒ†ãƒ¼ãƒã¯ãã®ã¾ã¾ã«ã—ã¦ãã ã•ã„ã€‚

    """

    prompt = prompt + f"""**æ—¢å­˜ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿**ï¼š

    ```
    {conversation_text}
    ```

    """

    prompt = prompt + f"{old_persona_text}\n\n"
    prompt = prompt + f"{new_persona_text}\n\n"

    prompt = prompt + """**å‡ºåŠ›å½¢å¼**ï¼š

    ```
    User: [ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€]
    Assistant: [ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ç™ºè¨€]
    ...
    ```

    **æ³¨æ„äº‹é …**ï¼š

    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸€äººç§°ã‚„è¨€è‘‰é£ã„ã‚’æ–°ã—ã„ãƒšãƒ«ã‚½ãƒŠã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ€§æ ¼ã‚„è¶£å‘³ã‚’åæ˜ ã—ã€ç™ºè¨€å†…å®¹ã‚’è‡ªç„¶ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
    - **ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ç™ºè¨€ã‚‚ã€ä¼šè©±ã®è‡ªç„¶ã•ã‚’ä¿ã¤ãŸã‚ã«å¿…è¦ãªå ´åˆã®ã¿æœ€å°é™ã®å¤‰æ›´ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ãŸã ã—ã€å…ƒã®æ„å›³ã‚„æƒ…å ±ã¯ç¶­æŒã—ã¦ãã ã•ã„ã€‚**
    - ä½¿ç”¨ã§ãã‚‹çµµæ–‡å­—ã¯æ¬¡ã«æŒ™ã’ã‚‹ã‚‚ã®ã ã‘ã§ã™ã€‚ãã‚Œä»¥å¤–ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ã€ŒğŸ˜Š,ğŸ¥°,ğŸ˜‰,ğŸ¤—,ğŸ˜­,ğŸ¤£,ğŸ˜†,ğŸ™„,ğŸ˜¡,ğŸ˜²,ğŸ˜³,ğŸ˜”,ğŸ˜‡,ğŸ˜™,ğŸ¥³,ğŸ¤”,ğŸ¥º,ğŸ¥±,ğŸ’“,âœ¨ã€
    """

    system_prompt = "ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
    if model_name == "tanuki":
        system_prompt = "ä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚"

    temperature = 0.5
    if model_name == "qwen":
        temperature = 1.0

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
    output_ids = model.generate(input_ids,
                                max_new_tokens=1024,
                                do_sample=True,
                                temperature=temperature,
                                top_k=50,
                                top_p=0.9,
                                num_return_sequences=5
                                )

    for i, output_id in enumerate(output_ids):
        text = tokenizer.decode(output_id[input_ids.shape[-1]:], skip_special_tokens=True)

        with open(f"outputs/{model_name}/output_{i}.txt", "w") as f:
            f.write(text)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, required=True)
    args = parser.parse_args()

    main(args.model_name)
